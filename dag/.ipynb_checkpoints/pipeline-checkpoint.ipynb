{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pypi.org/project/pymysql-pooling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'date_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b8fe1b828bf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-b8fe1b828bf3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;31m#     if \"timesheets\" in args.csvfile:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         run_timesheet_pipeline(\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mcsv_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"timesheets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m \u001b[0;31m#     elif \"employee\" in args.csvfile:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'date_filter' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import schema\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_schema\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from helper.utils import check_decimal, check_int, check_date, check_timeformat\n",
    "from pandas_schema.validation import CustomElementValidation\n",
    "from pandas_schema import Column\n",
    "from dask import dataframe as dd\n",
    "from database.base_operation import DBConn, DBOperations\n",
    "from datetime import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "conn = DBConn()\n",
    "db_opr = DBOperations(conn)\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    def __init__(self, date_=None):\n",
    "        self.data = None\n",
    "        self.date_filter = None\n",
    "\n",
    "    def read_all_data(self, finput) -> dd:\n",
    "        \"\"\"\n",
    "        Read csv file and return data based on date_filter\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        self.data = dd.read_csv(finput)\n",
    "        end = time.time()\n",
    "        print(\"Extract csv with dask:\", round((end - start), 5), \"sec\")\n",
    "\n",
    "    def read_data_based_key(\n",
    "        self, finput=None, key_filter=None, date_filter=None\n",
    "    ) -> dd:\n",
    "        \"\"\"\n",
    "        Read csv file and return data based on date_filter\n",
    "        \"\"\"\n",
    "        self.date_filter = date_filter\n",
    "        start = time.time()\n",
    "        self.data = dd.read_csv(finput)\n",
    "        self.data = self.data[self.data[key_filter] == date_filter]\n",
    "        end = time.time()\n",
    "        print(\"Extract csv with dask:\", round((end - start), 5), \"sec\")\n",
    "\n",
    "    def remove_duplicate(self, keys=None) -> dd:\n",
    "        \"\"\"\n",
    "        Remove duplicate record based on key(s)\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "\n",
    "        if len(keys) > 1:\n",
    "            s_key = \"_\".join(keys)\n",
    "            self.data[s_key] = (\n",
    "                self.data[keys[0]].astype(str)\n",
    "                + \"_\"\n",
    "                + self.data[keys[1]].astype(str)\n",
    "            )\n",
    "            self.data = self.data.drop_duplicates(\n",
    "                subset=\"%s_%s\" % (keys[0], keys[1])\n",
    "            )\n",
    "            self.data = self.data.drop(\"%s_%s\" % (keys[0], keys[1]), axis=1)\n",
    "        elif len(keys) == 1:\n",
    "            self.data = self.data.drop_duplicates(subset=keys)\n",
    "            end = time.time()\n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Remove duplicate:\", round((end - start), 5), \"sec\")\n",
    "        self.data = self.data.compute()\n",
    "\n",
    "    def __get_schema(self, fname):\n",
    "        decimal_validation = CustomElementValidation(\n",
    "            lambda d: check_decimal(d), \"is not decimal\"\n",
    "        )\n",
    "        int_validation = CustomElementValidation(\n",
    "            lambda i: check_int(i), \"is not integer\"\n",
    "        )\n",
    "        date_validation = CustomElementValidation(\n",
    "            lambda i: check_date(i),\n",
    "            \"is incorrect date string format. It should be YYYY-MM-DD\",\n",
    "        )\n",
    "        time_validation = CustomElementValidation(\n",
    "            lambda t: check_timeformat(t), \"is not time\"\n",
    "        )\n",
    "        null_validation = CustomElementValidation(\n",
    "            lambda d: d is not np.nan, \"this field cannot be null\"\n",
    "        )\n",
    "\n",
    "        columns = list()\n",
    "        with open(f\"schema/{fname}.json\", \"r\") as fin:\n",
    "            for field in json.load(fin):\n",
    "                validations = list()\n",
    "                if field[\"type\"] == \"INT\":\n",
    "                    validations.append(int_validation)\n",
    "                elif field[\"type\"] == \"DATE\":\n",
    "                    validations.append(date_validation)\n",
    "                elif field[\"type\"] == \"DECIMAL\":\n",
    "                    validations.append(decimal_validation)\n",
    "                elif field[\"type\"] == \"TIME\":\n",
    "                    validations.append(time_validation)\n",
    "\n",
    "                if field[\"mode\"] == \"REQUIRED\":\n",
    "                    validations.append(null_validation)\n",
    "\n",
    "                columns.append(Column(field[\"name\"], validations))\n",
    "        return pandas_schema.Schema(columns)\n",
    "\n",
    "    def validate_data(self, fname=None):\n",
    "        schema = self.__get_schema(fname)\n",
    "        errors = schema.validate(self.data)\n",
    "        if errors:\n",
    "            raise ValueError(\"\\n\" + \"\\n\".join([str(e) for e in errors]))\n",
    "        return self.data\n",
    "\n",
    "\n",
    "def run_timesheet_pipeline(csv_file=None, date_filter=None):\n",
    "    assert csv_file != None, \"Missing argument required (csv_file)...\"\n",
    "    if date_filter:\n",
    "        assert datetime.strptime(\n",
    "            date_filter, \"%Y-%m-%d\"\n",
    "        ), \"Filter date not match with required format. Please input YYYY-MM-DD\"\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "    if date_filter:\n",
    "        pipeline.read_data_based_key(\n",
    "            f\"../data/{csv_file}\", \"date\", date_filter\n",
    "        )\n",
    "    else:\n",
    "        pipeline.read_all_data(f\"../data/{csv_file}\")\n",
    "\n",
    "    pipeline.remove_duplicate([\"employee_id\", \"date\"])\n",
    "    if pipeline.data.empty:\n",
    "        print(\"No data found...\")\n",
    "    else:\n",
    "        final_data = pipeline.validate_data(\"timesheets\")\n",
    "        db_opr.insert_timesheet_record(final_data)\n",
    "        print(f\"DONE!!! {len(final_data)} of processed data\")\n",
    "\n",
    "\n",
    "def run_employee_pipeline(csv_file=None, date_filter=None):\n",
    "    assert csv_file != None, \"Missing argument required (csv_file)...\"\n",
    "    if date_filter:\n",
    "        assert datetime.strptime(\n",
    "            date_filter, \"%Y-%m-%d\"\n",
    "        ), \"Filter date not match with required format. Please input YYYY-MM-DD\"\n",
    "\n",
    "    pipeline = Pipeline()\n",
    "\n",
    "    if date_filter:\n",
    "        pipeline.read_data_based_key(\n",
    "            f\"../data/{csv_file}\", \"join_date\", date_filter\n",
    "        )\n",
    "    else:\n",
    "        pipeline.read_all_data(f\"../data/{csv_file}\")\n",
    "\n",
    "    pipeline.remove_duplicate([\"employe_id\"])\n",
    "    if pipeline.data.empty:\n",
    "        print(\"No data found...\")\n",
    "    else:\n",
    "        final_data = pipeline.validate_data(\"employee\")\n",
    "        db_opr.insert_employees_record(final_data)\n",
    "        print(f\"DONE!!! {len(final_data)} of processed data\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    yesterday = date.today() - timedelta(days=1)\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         \"--csvfile\",\n",
    "#         type=str,\n",
    "#         default=\"timesheets.csv\",\n",
    "#         help=\"File to be uploaded in .csv\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--datefilter\",\n",
    "#         type=str,\n",
    "#         default=\"data-training-cleaned-v1\",\n",
    "#         help=\"Table destination\",\n",
    "#     )\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if not (\".csv\" in args.csvfile):\n",
    "#         raise Exception(\"Source data didn't not found in folder 'data' !!!\")\n",
    "\n",
    "#     if \"timesheets\" in args.csvfile:\n",
    "        run_timesheet_pipeline(\n",
    "            csv_file=\"timesheets.csv\", date_filter=date_filter\n",
    "        )\n",
    "#     elif \"employee\" in args.csvfile:\n",
    "        run_employee_pipeline(\n",
    "            csv_file=\"employees.csv\", date_filter=date_filter\n",
    "        )\n",
    "#     else:\n",
    "#         raise Exception(\n",
    "#             \"Didn't match to any pipeline. Input employees or timesheets instead.\"\n",
    "#         )\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
