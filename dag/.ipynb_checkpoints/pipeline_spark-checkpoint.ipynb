{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_int(num):\n",
    "    try:\n",
    "        int(num)\n",
    "    except ValueError:\n",
    "        return f'VALUE ERROR -> {num}'\n",
    "    return num\n",
    "\n",
    "def check_date(date_):\n",
    "    try:\n",
    "        if date_ is not None:\n",
    "            datetime.strptime(date_, \"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        return f'VALUE ERROR -> {date_}'\n",
    "    return date_\n",
    "\n",
    "def check_timeformat(time_):\n",
    "    try:\n",
    "        if time_ is not None:\n",
    "            datetime.strptime(time_, \"%H:%M:%S\")\n",
    "    except ValueError:\n",
    "        return f'VALUE ERROR -> {time_}'\n",
    "    return time_\n",
    "\n",
    "def validate_data_timesheet(df):\n",
    "    NumberCheckerUDF = udf(lambda element: check_int(element))\n",
    "    DateCheckerUDF = udf(lambda element: check_date(element))\n",
    "    TimeCheckerUDF = udf(lambda element: check_timeformat(element))\n",
    "    df = (\n",
    "        df.withColumn(\"timesheet_id\", NumberCheckerUDF(\"timesheet_id\"))\n",
    "        .withColumn(\"employee_id\", NumberCheckerUDF(\"employee_id\"))\n",
    "        .withColumn(\"date\", DateCheckerUDF(\"date\"))\n",
    "        .withColumn(\"checkin\", TimeCheckerUDF(\"checkin\"))\n",
    "        .withColumn(\"checkout\", TimeCheckerUDF(\"checkout\"))\n",
    "    )\n",
    "    df_errors = df.filter(\n",
    "        (df.timesheet_id.like ('VALUE ERROR%'))\n",
    "        | (df.employee_id.like('VALUE ERROR%'))\n",
    "        | (df.date.like('VALUE ERROR%'))\n",
    "        | (df.checkin.like('VALUE ERROR%'))\n",
    "        | (df.checkout.like('VALUE ERROR%'))\n",
    "    ).collect()\n",
    "    \n",
    "    if len(df_errors) > 0:\n",
    "        raise ValueError (f\"Incorrect Data Type Found\\n {df_errors}\") \n",
    "\n",
    "    return df\n",
    "\n",
    "def validate_data_employee(df):\n",
    "    NumberCheckerUDF = udf(lambda element: check_int(element))\n",
    "    DateCheckerUDF = udf(lambda element: check_date(element))\n",
    "    TimeCheckerUDF = udf(lambda element: check_timeformat(element))\n",
    "    df = (\n",
    "        df.withColumn(\"employe_id\", NumberCheckerUDF(\"employe_id\"))\n",
    "        .withColumn(\"branch_id\", NumberCheckerUDF(\"branch_id\"))\n",
    "        .withColumn(\"salary\", NumberCheckerUDF(\"salary\"))\n",
    "        .withColumn(\"join_date\", DateCheckerUDF(\"join_date\"))\n",
    "        .withColumn(\"resign_date\", DateCheckerUDF(\"resign_date\"))\n",
    "    )\n",
    "    df_errors = df.filter(\n",
    "        (df.employe_id.like ('VALUE ERROR%'))\n",
    "        | (df.branch_id.like('VALUE ERROR%'))\n",
    "        | (df.salary.like('VALUE ERROR%'))\n",
    "        | (df.join_date.like('VALUE ERROR%'))\n",
    "        | (df.resign_date.like('VALUE ERROR%'))\n",
    "    ).collect()\n",
    "    \n",
    "    if len(df_errors) > 0:\n",
    "        raise ValueError (f\"Incorrect Data Type Found\\n {df_errors}\") \n",
    "    return df\n",
    "\n",
    "def calculate_work_duration(checkin,checkout):\n",
    "    if checkin < checkout:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_with_schema(spark, file_path):\n",
    "    df = spark.read.csv(file_path, header=True, sep=\",\").cache()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pipeline_employee(spark, date_filter=None):\n",
    "    file_path = '../data/employees.csv'\n",
    "    df = load_file_with_schema(spark, file_path)\n",
    "    \n",
    "    if date_filter:\n",
    "        df = df.filter(df.join_date == date_filter)\n",
    "    print('Total Records = {}'.format(df.count()))\n",
    "\n",
    "    if df.count() == 0:\n",
    "        print(\"No data found ...\")\n",
    "    else:\n",
    "        df = validate_data_employee(df)\n",
    "        df = df.drop_duplicates()\n",
    "    return df\n",
    "        \n",
    "        \n",
    "def pipeline_timesheet(spark, date_filter=None):\n",
    "    file_path = '../data/timesheets.csv'\n",
    "    df = load_file_with_schema(spark, file_path)\n",
    "    \n",
    "    if date_filter:\n",
    "        df = df.filter(df.date == date_filter)\n",
    "    print('Total Records = {}'.format(df.count()))\n",
    "    \n",
    "    if df.count() == 1:\n",
    "        print(\"No data found ...\")\n",
    "    else:\n",
    "        df = validate_data_timesheet(df)\n",
    "        df = df.drop_duplicates(subset= ['employee_id', 'date'])\n",
    "#         df = df.withColumn('work_hour', calculate_work_duration('checkin','checkout'))    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records = 2\n",
      "Total Records = 5\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "appName = \"etl\"\n",
    "master = \"local\"\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(appName) \\\n",
    "    .setMaster(master) \n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession\n",
    "\n",
    "df_employee = pipeline_employee(spark, date_filter = '2020-12-07')\n",
    "df_timesheet = pipeline_timesheet(spark, date_filter = '2019-08-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_to_db(df):\n",
    "    prop = {'user': 'root',\n",
    "            'password': 'root',\n",
    "            'driver': 'com.mysql.jdbc.Driver'\n",
    "           }\n",
    "    url = 'jdbc:mysql://localhost:3306/employees'\n",
    "\n",
    "    df.write.jdbc(url=url, table='employee', mode='append', properties=prop)\n",
    "\n",
    "save_to_db(df_employee)\n",
    "save_to_db(df_timesheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+----------+-----------+\n",
      "|employe_id|branch_id|  salary| join_date|resign_date|\n",
      "+----------+---------+--------+----------+-----------+\n",
      "|    331374|     2629| 6000000|2020-12-07|       null|\n",
      "|    329501|        1|13000000|2020-12-07|       null|\n",
      "+----------+---------+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+----------+--------+--------+\n",
      "|timesheet_id|employee_id|      date| checkin|checkout|\n",
      "+------------+-----------+----------+--------+--------+\n",
      "|    23907437|         60|2019-08-21|09:56:05|17:31:08|\n",
      "|    23907432|         66|2019-08-21|08:13:31|17:05:02|\n",
      "|    23907434|         21|2019-08-21|09:45:08|18:24:06|\n",
      "|    23907433|         22|2019-08-21|08:56:34|18:00:48|\n",
      "|    23907435|         63|2019-08-21|09:55:47|    null|\n",
      "+------------+-----------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_timesheet.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
