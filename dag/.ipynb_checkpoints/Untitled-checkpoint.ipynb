{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_with_schema(spark, file_path):\n",
    "    df = spark.read.csv(file_path, header=True, sep=\",\").cache()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_int(num):\n",
    "    try:\n",
    "        int(num)\n",
    "    except ValueError:\n",
    "        return f'VALUE ERROR -> {num}'\n",
    "    return num\n",
    "\n",
    "def check_date(date_):\n",
    "    try:\n",
    "        if date_ is not None:\n",
    "            datetime.strptime(date_, \"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        return f'VALUE ERROR -> {date_}'\n",
    "    return date_\n",
    "\n",
    "def check_timeformat(time_):\n",
    "    try:\n",
    "        if time_ is not None:\n",
    "            datetime.strptime(time_, \"%H:%M:%S\")\n",
    "    except ValueError:\n",
    "        return f'VALUE ERROR -> {time_}'\n",
    "    return time_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_data_timesheet(df):\n",
    "    NumberCheckerUDF = udf(lambda element: check_int(element))\n",
    "    DateCheckerUDF = udf(lambda element: check_date(element))\n",
    "    TimeCheckerUDF = udf(lambda element: check_timeformat(element))\n",
    "    df = (\n",
    "        df.withColumn(\"timesheet_id\", NumberCheckerUDF(\"timesheet_id\"))\n",
    "        .withColumn(\"employee_id\", NumberCheckerUDF(\"employee_id\"))\n",
    "        .withColumn(\"date\", DateCheckerUDF(\"date\"))\n",
    "        .withColumn(\"checkin\", TimeCheckerUDF(\"checkin\"))\n",
    "        .withColumn(\"checkout\", TimeCheckerUDF(\"checkout\"))\n",
    "    )\n",
    "    df_errors = df.filter(\n",
    "        (df.timesheet_id.like ('VALUE ERROR%'))\n",
    "        | (df.employee_id.like('VALUE ERROR%'))\n",
    "        | (df.date.like('VALUE ERROR%'))\n",
    "        | (df.checkin.like('VALUE ERROR%'))\n",
    "        | (df.checkout.like('VALUE ERROR%'))\n",
    "    ).collect()\n",
    "    \n",
    "    if len(df_errors) > 0:\n",
    "        raise ValueError (f\"Incorrect Data Type Found\\n {df_errors}\") \n",
    "\n",
    "    return df\n",
    "\n",
    "def validate_data_employee(df):\n",
    "    NumberCheckerUDF = udf(lambda element: check_int(element))\n",
    "    DateCheckerUDF = udf(lambda element: check_date(element))\n",
    "    TimeCheckerUDF = udf(lambda element: check_timeformat(element))\n",
    "    df = (\n",
    "        df.withColumn(\"employe_id\", NumberCheckerUDF(\"employe_id\"))\n",
    "        .withColumn(\"branch_id\", NumberCheckerUDF(\"branch_id\"))\n",
    "        .withColumn(\"salary\", NumberCheckerUDF(\"salary\"))\n",
    "        .withColumn(\"join_date\", DateCheckerUDF(\"join_date\"))\n",
    "        .withColumn(\"resign_date\", DateCheckerUDF(\"resign_date\"))\n",
    "    )\n",
    "    df_errors = df.filter(\n",
    "        (df.employe_id.like ('VALUE ERROR%'))\n",
    "        | (df.branch_id.like('VALUE ERROR%'))\n",
    "        | (df.salary.like('VALUE ERROR%'))\n",
    "        | (df.join_date.like('VALUE ERROR%'))\n",
    "        | (df.resign_date.like('VALUE ERROR%'))\n",
    "    ).collect()\n",
    "    \n",
    "    if len(df_errors) > 0:\n",
    "        raise ValueError (f\"Incorrect Data Type Found\\n {df_errors}\") \n",
    "    return df\n",
    "\n",
    "def calculate_work_duration(checkin,checkout):\n",
    "    if checkin < checkout:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pipeline_employee(spark, date_filter=None):\n",
    "    file_path = '../data/employees.csv'\n",
    "    df = load_file_with_schema(spark, file_path)\n",
    "    \n",
    "    if date_filter:\n",
    "        df = df.filter(df.join_date == date_filter)\n",
    "    print('Total Records = {}'.format(df.count()))\n",
    "\n",
    "    if df.count() == 0:\n",
    "        print(\"No data found ...\")\n",
    "    else:\n",
    "        df = validate_data_employee(df)\n",
    "        df = df.drop_duplicates()\n",
    "    return df\n",
    "        \n",
    "        \n",
    "def pipeline_timesheet(spark, date_filter=None):\n",
    "    file_path = '../data/timesheets.csv'\n",
    "    df = load_file_with_schema(spark, file_path)\n",
    "    \n",
    "    if date_filter:\n",
    "        df = df.filter(df.date == date_filter)\n",
    "    print('Total Records = {}'.format(df.count()))\n",
    "    \n",
    "    if df.count() == 1:\n",
    "        print(\"No data found ...\")\n",
    "    else:\n",
    "        df = validate_data_timesheet(df)\n",
    "        df = df.drop_duplicates(subset= ['employee_id', 'date'])\n",
    "#         df = df.withColumn('work_hour', calculate_work_duration('checkin','checkout'))    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "appName = \"etl\"\n",
    "master = \"local\"\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(appName) \\\n",
    "    .setMaster(master) \\\n",
    "#     .set(\"spark.driver.extraClassPath\",\"sqljdbc_7.2/enu/mssql-jdbc-7.2.2.jre8.jar\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession\n",
    "\n",
    "df = pipeline_employee(spark, date_filter = '2020-12-07')\n",
    "# df = pipeline_timesheet(spark, date_filter = '2019-08-21')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prop = {'user': 'root',\n",
    "        'password': 'csui6803',\n",
    "        'driver': 'com.mysql.jdbc.Driver'\n",
    "       }\n",
    "\n",
    "# # database address (need to be modified)\n",
    "url = 'jdbc:mysql://localhost:3306/employees'\n",
    "\n",
    "df.write.jdbc(url=url, table='employee', mode='append', properties=prop)\n",
    "# sc.stop()\n",
    "df.write.format('jdbc').options(\n",
    "      url='jdbc:mysql://localhost/employees',\n",
    "      driver='com.mysql.jdbc.Driver',\n",
    "#       driver = 'com.mysql.cj.jdbc.Driver',\n",
    "      dbtable='employee',\n",
    "      user='root',\n",
    "      password='csui6803').mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
